{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPIP3ziFqZMjFS62bLJCxmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertGrados/RobertGrados/blob/main/HW8_RG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the comprehensive dataset provided from Zillow, which encompasses various features ranging from architectural details to taxation information, students are tasked to\n",
        "# apply unsupervised learning methods to uncover underlying patterns in the data. The\n",
        "# assignment will involve two main techniques: Principal Component Analysis (PCA) and\n",
        "# Clustering."
      ],
      "metadata": {
        "id": "m_hGQ2fdK-MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cK-h12QXMVxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6tyzgwGKyfR"
      },
      "outputs": [],
      "source": [
        "!pip install -U dataprep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataprep.eda import plot, plot_correlation, plot_missing\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "gEBK2YE13OlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Homework8.csv\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "sIJ6jolUMSoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataprep.eda import plot, plot_correlation, plot_missing\n",
        "plot(df['merchant'])"
      ],
      "metadata": {
        "id": "NiYIsmZNN2e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Homework8.csv\", index_col='time',parse_dates=True)"
      ],
      "metadata": {
        "id": "m3m4fc5RN8Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(df.index >= \"2033-01-01\") & (df.index < \"2035-01-01\")]"
      ],
      "metadata": {
        "id": "Xf1Hhip_PBdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_transactions = df.resample(\"D\")[\"amount_usd_in_cents\"].sum()\n",
        "# Save the resampled data (optinal)\n",
        "daily_transactions.to_csv(\"daily_transactions.csv\")"
      ],
      "metadata": {
        "id": "I4jdArwUPXD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXg6UBgxEn1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D001nKdDEnow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(daily_transactions.head())"
      ],
      "metadata": {
        "id": "k2xgQoW4QFYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data=daily_transactions\n",
        "\n",
        "#Define the model\n",
        "model = ExponentialSmoothing(data, seasonal='mul', seasonal_periods=7, trend=\"additive\")  # Adjust for weekly seasonality (7 days)\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Forecast future n days (replace 30 with your desired number)\n",
        "futere_dates = data.index[-1] + pd.DateOffset(days=30) # Add 30 days to the Last date\n",
        "forecast = model_fit.forecast(steps=30)\n",
        "\n",
        "print(forecast)\n",
        "forecast.plot(label=\"Forecast\")\n",
        "data.plot(label=\"Actual\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kqhy4wcC81hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data=daily_transactions\n",
        "\n",
        "# Holt-Winters model (same as before)\n",
        "model = ExponentialSmoothing(data, seasonal='add', seasonal_periods=7)\n",
        "model_fit = model.fit()\n",
        "forecast = model_fit.forecast(steps=30)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6)) # Adjust figure size if needed\n",
        "plt.plot(data, label='Historical Data',linestyle='-')\n",
        "plt.plot(forecast, label='Forecast',linestyle='--')\n",
        "plt.title('Daily Transaction Data and Holt-Winters Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Transaction Sum (Dollars)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yuc5APTCAR1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing  # Assuming you're using Holt-Winters\n",
        "\n",
        "\n",
        "daily_data = daily_transactions\n",
        "\n",
        "# Define window size and number of folds\n",
        "window_size = 30  # Adjust window size as needed\n",
        "folds = 5\n",
        "\n",
        "# Create time series split object\n",
        "cv = TimeSeriesSplit(n_splits=folds)\n",
        "\n",
        "# Evaluate model performance (e.g., using Mean Squared Error)\n",
        "mse_scores = []\n",
        "for train, test in cv.split(daily_data):\n",
        "    model = ExponentialSmoothing(daily_data[train], seasonal='add', seasonal_periods=7)\n",
        "    model_fit = model.fit()\n",
        "    forecast = model_fit.forecast(steps=len(test))\n",
        "    mse = ((daily_data[test] - forecast)**2).mean()  # Mean Squared Error\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Print average MSE across folds\n",
        "average_mse = sum(mse_scores) / len(mse_scores)\n",
        "print(f\"Average Mean Squared Error (MSE) for Holt-Winters: {average_mse}\")"
      ],
      "metadata": {
        "id": "rXjWlB4cANWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of lags to consider\n",
        "lags = 7  # Adjust as needed\n",
        "\n",
        "# Create a DataFrame to store lagged features\n",
        "df_features = pd.DataFrame()\n",
        "df_features['original_data'] = daily_transactions\n",
        "\n",
        "# Add lagged features\n",
        "for i in range(1, lags + 1):\n",
        "    df_features[f'lag_{i}'] = daily_data.shift(i)\n",
        "\n",
        "# Drop the first 'lags' rows (contain NaN values due to shifting)\n",
        "df_features = df_features.dropna()\n",
        "\n",
        "# Separate features and target variable (assuming daily_transactions is the target)\n",
        "X = df_features.drop('original_data', axis=1)\n",
        "y = df_features['original_data']\n",
        "\n",
        "# Select features to plot (adjust the number based on your 'lags')\n",
        "features_to_plot = ['original_data', 'lag_1', 'lag_2', 'lag_3']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "for feature in features_to_plot:\n",
        "    plt.plot(df_features[feature], label=feature)\n",
        "\n",
        "plt.title('Original Transaction Data and Lagged Features')\n",
        "plt.xlabel('Time Index')\n",
        "plt.ylabel('Transaction Sum (Dollars)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FpoS3SXa-nP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... your code for loading data and creating lagged features (df_features)\n",
        "data = pd.read_csv('daily_transactions.csv')\n",
        "\n",
        "# Define how many lags to create\n",
        "lag_periods = [1, 7, 30]\n",
        "\n",
        "# Create lagged features\n",
        "for lag in lag_periods:\n",
        "  data[f'transction_lag_{lag}'] = data[\"amount_usd_in_cents\"].shift(lag)\n",
        "  # Drop initial row with missing values (NaN)\n",
        "  data = data.dropna()\n",
        "\n",
        "data.head()\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_features.drop('original_data', axis=1)\n",
        "y = df_features['original_data']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Linear Regression Mean Squared Error (MSE): {mse}')\n",
        "\n",
        "# Plotting predictions vs. actual values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test, label='Actual')\n",
        "plt.plot(y_pred, label='Predicted')\n",
        "plt.title('Linear Regression: Actual vs. Predicted Transactions')\n",
        "plt.xlabel('Time Index')\n",
        "plt.ylabel('Transaction Sum (Dollars)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ifbsoEQRDGFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forcasting Methods\n",
        "!python -m pip install statsmodels\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "cv =TimeSeriesSplit(n_splits=3)\n",
        "mae_scores = []\n",
        "\n",
        "def holt_winters_forecast(steps=len(test_data))\n",
        "return"
      ],
      "metadata": {
        "id": "FSgESJxf8Rc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4VFHjxD41mo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}